{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Recognizer (with Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in training set: 60000\n",
      "Total images in test set: 10000\n",
      "Image dimension: 28 X 28\n",
      "Max pixel val: 255\n",
      "Min pixel val: 0\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"Total images in training set: %d\"%len(X_train))\n",
    "print(\"Total images in test set: %d\"%len(X_test))\n",
    "print(\"Image dimension: %d X %d\"%(X_train[0].shape[0],X_train[0].shape[1]))\n",
    "print(\"Max pixel val: %d\"%np.max(X_train))\n",
    "print(\"Min pixel val: %d\"%np.min(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot first 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADTCAYAAACRDeixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmUXCAd3MLBIO6GZmkXBANzOLxP8BsS1Ia26rPcwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b07c85ac88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(2,5,i+1)\n",
    "    ax.axis('off')\n",
    "    ax.imshow(X_train[i],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "[2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5])\n",
    "print(y_train[5:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All pixel values normalized between 0 and 1\n",
    "normalize = lambda x: x/255.0\n",
    "v_normalize = np.vectorize(normalize)\n",
    "\n",
    "X_train_processed_norm = v_normalize(X_train)\n",
    "X_test_processed_norm = v_normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding: Image labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "y_train_encoded = np_utils.to_categorical(y_train, 10)\n",
    "y_test_encoded = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 10 encoded image labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_encoded[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use GridSearch to tune hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model(dropout_prob=0.0, num_layers=1):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    \n",
    "    for i in range(1,num_layers):\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(dropout_prob))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy: 0.977283 using {'dropout_prob': 0.2, 'num_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=nn_model, epochs=10, batch_size=100, verbose=0)\n",
    "\n",
    "dropout_prob = [0.2,0.3,0.4]\n",
    "num_layers = [2,3,4]\n",
    "\n",
    "param_grid = dict(dropout_prob=dropout_prob, num_layers=num_layers)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_processed_norm, y_train_encoded)\n",
    "\n",
    "print(\"Best validation accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune batch-size using hyper-parameters obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy: 0.977717 using {'dropout_prob': 0.2, 'num_layers': 2, 'batch_size': 150}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=nn_model, epochs=10, verbose=0)\n",
    "\n",
    "dropout_prob = [0.2]\n",
    "num_layers = [2]\n",
    "batch_size = [50,100,150,10000,30000]\n",
    "\n",
    "param_grid = dict(dropout_prob=dropout_prob, num_layers=num_layers, batch_size=batch_size)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_processed_norm, y_train_encoded)\n",
    "\n",
    "print(\"Best validation accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune number of epochs using the tuned value of other hyper-parameters obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_45 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00000: val_loss improved from inf to 0.15017, saving model to mnist.model.best.hdf5\n",
      "Epoch 00001: val_loss improved from 0.15017 to 0.11929, saving model to mnist.model.best.hdf5\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 00003: val_loss improved from 0.11929 to 0.10231, saving model to mnist.model.best.hdf5\n",
      "Epoch 00004: val_loss improved from 0.10231 to 0.10129, saving model to mnist.model.best.hdf5\n",
      "Epoch 00005: val_loss improved from 0.10129 to 0.09608, saving model to mnist.model.best.hdf5\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 00029: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.best.hdf5',verbose=1, save_best_only=True)\n",
    "\n",
    "results = model.fit(X_train_processed_norm, y_train_encoded, batch_size=150, epochs=30,\n",
    "          validation_split=0.33, callbacks=[checkpointer],\n",
    "          verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.978600\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('mnist.model.best.hdf5')\n",
    "score = model.evaluate(X_test_processed_norm, y_test_encoded, verbose=0)\n",
    "print('Test accuracy: %f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_processed_norm = X_train_processed_norm.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test_processed_norm = X_test_processed_norm.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model(kernel_size=2, num_filters=1, num_dense_layers=1, dropout_prob=0.0):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=num_filters, kernel_size=kernel_size, strides=1, activation='relu',input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=num_filters, kernel_size=kernel_size, strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "        \n",
    "    for i in range(1,num_dense_layers):\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(dropout_prob))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use GridSearch to tune hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy: 0.988133 using {'dropout_prob': 0.2, 'num_dense_layers': 2, 'kernel_size': 3, 'num_filters': 16}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=cnn_model, epochs=10, batch_size=150, verbose=0)\n",
    "\n",
    "kernel_size = [2,3]\n",
    "num_filters = [8,16]\n",
    "num_dense_layers = [2,3]\n",
    "dropout_prob = [0.2,0.3,0.4]\n",
    "\n",
    "param_grid = dict(kernel_size=kernel_size, num_filters=num_filters, num_dense_layers=num_dense_layers, dropout_prob=dropout_prob)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_processed_norm, y_train_encoded)\n",
    "\n",
    "print(\"Best validation accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune number of epochs using the tuned value of other hyper-parameters obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_157 (Conv2D)          (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_157 (MaxPoolin (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_158 (Conv2D)          (None, 11, 11, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_158 (MaxPoolin (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_119 (Flatten)        (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_433 (Dense)            (None, 512)               205312    \n",
      "_________________________________________________________________\n",
      "dropout_310 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_434 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_311 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_435 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 475,578\n",
      "Trainable params: 475,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00000: val_loss improved from inf to 0.08921, saving model to mnist.cnn_model.best.hdf5\n",
      "Epoch 00001: val_loss improved from 0.08921 to 0.06392, saving model to mnist.cnn_model.best.hdf5\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 00004: val_loss improved from 0.06392 to 0.05175, saving model to mnist.cnn_model.best.hdf5\n",
      "Epoch 00005: val_loss improved from 0.05175 to 0.04714, saving model to mnist.cnn_model.best.hdf5\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00008: val_loss improved from 0.04714 to 0.04640, saving model to mnist.cnn_model.best.hdf5\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 00029: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=3, strides=1, activation='relu',input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=16, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.cnn_model.best.hdf5',verbose=1, save_best_only=True)\n",
    "\n",
    "results = model.fit(X_train_processed_norm, y_train_encoded, batch_size=150, epochs=30,\n",
    "          validation_split=0.33, callbacks=[checkpointer],\n",
    "          verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.990000\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('mnist.cnn_model.best.hdf5')\n",
    "score = model.evaluate(X_test_processed_norm, y_test_encoded, verbose=0)\n",
    "print('Test accuracy: %f' % score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
